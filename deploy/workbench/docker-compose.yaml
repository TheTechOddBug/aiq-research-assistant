services:
  aira-instruct-llm:
    container_name: aira-instruct-llm
    image: nvcr.io/nim/meta/llama-3.3-70b-instruct:1.14.0
    volumes:
    - ${MODEL_DIRECTORY:-/tmp}:/opt/nim/.cache
    user: "${USERID}"
    ports:
    - "8050:8000"
    expose:
    - "8050"
    environment:
      NGC_API_KEY: ${NVIDIA_API_KEY}
    shm_size: 20gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # RAG uses 0,1 so we assign 2,3 to the LLM
              device_ids: ['${AIRA_LLM_MS_GPU_ID_0:-2}', '${AIRA_LLM_MS_GPU_ID_1:-3}']
              capabilities: [gpu] 
    healthcheck:
      test: ["CMD", "python3", "-c", "import requests; requests.get('http://localhost:8000/v1/health/ready')"]
      interval: 30s
      timeout: 20s
      retries: 100
    networks:
      - nvidia-rag
    profiles: ["aira-gpu"]

  aira-backend-no-gpu: &aira-backend-no-gpu
    hostname: aira-backend
    container_name: aira-backend
    image: nvcr.io/nvidia/blueprint/aira-backend:v1.2.0
    build:
      context: ../../
      dockerfile: aira/Dockerfile
    entrypoint: "/entrypoint.sh"
    ports:
      - "3838:3838"
    expose:
      - "3838"
    environment:
      TAVILY_API_KEY: ${TAVILY_API_KEY:-this-is-a-test-key}
      AIRA_APPLY_GUARDRAIL: "false"
      OPENAI_API_KEY: ${NVIDIA_API_KEY:-your-nvidia-api-key}
      AIRA_HOSTED_NIMS: ${AIRA_HOSTED_NIMS:-true}
      # Required for rag middleware via fastapi extensions
      RAG_INGEST_URL: ${RAG_INGEST_URL:-http://ingestor-server:8082/v1}
      RAG_SERVER_URL: ${RAG_SERVER_URL:-http://rag-server:8081/v1}
    volumes:
      - ../../configs:/app/configs
    networks:
      - nvidia-rag
    profiles: ["aira-no-gpu"]

  aira-backend-gpu: 
    <<: *aira-backend-no-gpu
    environment:
      AIRA_HOSTED_NIMS: ${AIRA_HOSTED_NIMS:-false}
    profiles: ["aira-gpu"]
    
  aira-frontend:
    container_name: aira-frontend
    image: nvcr.io/nvidia/blueprint/aira-frontend:v1.2.0
    ports:
      - "3000:3000"
    expose:
      - "3000"
    networks:
      - nvidia-rag
    environment:
      NVWB_TRIM_PREFIX: true
      INFERENCE_ORIGIN: ${INFERENCE_ORIGIN:-http://aira-backend:3838}
    profiles: ["aira-no-gpu", "aira-gpu"]

  aira-load-files:
    image: nvcr.io/nvidia/blueprint/aira-load-files:v1.2.0
    environment:
      - RAG_INGEST_URL=http://${RAG_INGEST_URL:-ingestor-server}:8082/v1
      - PYTHONUNBUFFERED=1
    volumes:
      - /tmp:/tmp-data
    networks:
      - nvidia-rag
    profiles: ["load-default-files"]

# Use the nvidia-rag network created by the 
# RAG docker compose deployment
# If you are deploying RAG separately
# set external to false
networks:
  nvidia-rag:
    external: true
    name: nvidia-rag
