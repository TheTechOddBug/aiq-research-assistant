# SPDX-FileCopyrightText: Copyright (c) 2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

general:
  use_uvloop: true
  front_end:
    _type: fastapi
    endpoints:
      - path: /generate_query
        method: POST
        description: Creates the query
        function_name: generate_query
      - path: /generate_summary
        method: POST
        description: Generates the summary
        function_name: generate_summary
      - path: /artifact_qa
        method: POST
        description: Q/A or chat about a previously generated artifact
        function_name: artifact_qa
      - path: /aiqhealth
        method: GET
        description: Health check for the AIQ AIRA service
        function_name: health_check
      - path: /default_collections
        method: GET
        description: Get the default collections
        function_name: default_collections

        
  telemetry:
    logging:
      console:
        _type: console
        level: WARNING
    # Uncomment this if you want to use W&B Weave for tracing
    # tracing:
    #   weave:
    #     _type: weave
    #     project: "NAT-BP-Project-Default" # Name of the project in weave, runs will be grouped under this project name

llms:
  # The inst_llm is used for Q&A and report writing and should be an instruct model.
  # The default configuration below is assuming a docker compose deployment of AIRA 
  # that uses local NVIDIA NIM microservices.
  # Update if you are deploying differently or using hosted NVIDIA NIM microservices.
  instruct_llm:
    _type: nim
    model_name: meta/llama-3.3-70b-instruct
    temperature: 0.0
    base_url: ${INSTRUCT_LLM_BASE_URL:-http://aira-instruct-llm:8000/v1}
    max_tokens: 20000
    api_key: not-needed
  # The reasoning llm is used for report planning and reflection and should be a reasoning model
  # that supports thinking tokens. The default configuration below is used assuming a docker compose
  # deployment of AIRA and RAG with a local NVIDIA NIM microservice for the nemotron model.
  # Update if you are deploying differently or using hosted NVIDIA NIM microservices.
  nemotron:
    _type: nim
    model_name : nvidia/llama-3.3-nemotron-super-49b-v1.5  # Both local and hosted deployment with dots format (nvidia/llama-3.3-nemotron-super-49b-v1.5)
    temperature: 0.5
    base_url: ${NEMOTRON_LLM_BASE_URL:-http://nim-llm-ms:8000/v1}
    disable_streaming: false # Disable streaming with nim models
    api_key: not-needed  # Uncomment this for hosted endpoint
    max_tokens: 5000

  eval_llm:
    _type: nim
    model_name: meta/llama-3.1-70b-instruct
    temperature: 0.0
    base_url: ${EVAL_LLM_BASE_URL:-https://integrate.api.nvidia.com/v1}
    api_key: ${NVIDIA_API_KEY}

  # Mistral is a good model for RAGAS metrics due to the variability it provides as opposed to the llama models
  ragas_llm:
    _type: nim
    model_name: mistralai/mixtral-8x22b-instruct-v0.1
    temperature: 0.0
    base_url: ${RAGAS_LLM_BASE_URL:-https://integrate.api.nvidia.com/v1}
    api_key: ${NVIDIA_API_KEY}

functions:
  generate_query:
    _type: generate_queries

  generate_summary:
    _type: generate_summaries
    # update to the IP address of the RAG server if you are not deploying RAG with docker compose
    rag_url: ${RAG_SERVER_URL:-http://rag-server:8081/v1}

  artifact_qa:
    _type: artifact_qa
    llm_name: instruct_llm
    # update to the IP address of the RAG server if you are not deploying RAG with docker compose (using one of our endpoints for now)
    rag_url: ${RAG_SERVER_URL:-http://rag-server:8081/v1}
    
  health_check:
    _type: health_check

  default_collections:
    _type: default_collections
    collections:
      - name: "Biomedical_Dataset"
        topic: "Biomedical"
        report_organization: "You are a medical researcher who specializes in cystic fibrosis. Create a report analyzing how CFTR modulators can be used to restore CFTR protein functions. Include a 150-200 word abstract and a methods, results, and discussion section. Format your answer in paragraphs. Consider all (and only) relevant data. Give a factual report with cited sources."
      - name: "Financial_Dataset"
        topic: "Financial"
        report_organization: "You are a financial analyst who specializes in financial statement analysis. Write a financial report analyzing the 2023 financial performance of Amazon. Identify trends in revenue growth, net income, and total assets. Discuss how these trends affected Amazon's yearly financial performance for 2023. Your output should be organized into a brief introduction, as many sections as necessary to create a comprehensive report, and a conclusion. Format your answer in paragraphs. Use factual sources such as Amazon's quarterly meeting releases for 2023. Cross analyze the sources to draw original and sound conclusions and explain your reasoning for arriving at conclusions. Do not make any false or unverifiable claims. I want a factual report with cited sources."


workflow:
  _type: aira_evaluator_workflow
  generator:
    _type: full
    verbose: true
    fact_extraction_llm: meta/llama-3.1-70b-instruct  # Use actual NIM model name
    citation_pairing_llm: mistralai/mixtral-8x22b-instruct-v0.1  # Use actual NIM model name, would recommend using the mixtral model for citation to fact pairing


# Evaluation configuration
eval:
  general:
    output_dir: ./.tmp/aiq_aira/
    cleanup: true
    workflow_alias: "Test_Run_Name" # This will set the run name in weave and where each individual run will be stored under this name

    dataset:
      _type: json
      file_path: data/eval_dataset.json
      id_key: id
      structure:
        disable: true
    profiler:
     base_metrics: true

  # Due to the current prompt structure, the mixtral model seems to be the best model for citation quality evaluation, feel free to change the model to your liking.
  evaluators:
    coverage:
      _type: coverage
      llm: eval_llm

    hallucination:
      _type: hallucination 
      llm: eval_llm

    synthesis:
      _type: synthesis
      llm: eval_llm
    
    citation_precision:
      _type: citation_precision
      llm: eval_llm

    citation_recall:
      _type: citation_recall
      llm: eval_llm

    citation_f1:
      _type: citation_f1
      llm: eval_llm

    rag_accuracy:
      _type: ragas_wrapper  
      metric: AnswerAccuracy
      llm: ragas_llm

    rag_groundedness:
      _type: ragas_wrapper
      metric: ResponseGroundedness
      llm: ragas_llm
      
    rag_relevance:
      _type: ragas_wrapper
      metric: ContextRelevance
      llm: ragas_llm